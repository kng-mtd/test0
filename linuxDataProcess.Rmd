---
title: "linuxDataProcessing"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    code_folding: show 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# データ処理やテキスト処理を行うための主要なコマンド

---

## **テキスト処理系コマンド**

### **1. cat（ファイル内容の表示）**
```bash
cat file.txt
```
複数ファイルの結合
```bash
cat file1.txt file2.txt > merged.txt
```

### **2. head（ファイルの先頭行を表示）**
```bash
head -n 10 file.txt  # 先頭10行を表示
```

### **3. tail（ファイルの末尾行を表示）**
```bash
tail -n 10 file.txt  # 末尾10行を表示
```
リアルタイムでファイルの更新を監視
```bash
tail -f log.txt
```

### **4. cut（特定の列を抽出）**
CSVの2列目を取得
```bash
cut -d ',' -f 2 file.csv
```
タブ区切りの3列目を取得
```bash
cut -d $'\t' -f 3 file.tsv
```

### **5. awk（列ごとの処理）**
CSVの2列目を取得
```bash
awk -F ',' '{print $2}' file.csv
```
条件付きで処理
```bash
awk -F ',' '$3 > 50 {print $1, $2}' file.csv
```

### **6. sed（文字列置換）**
「apple」を「orange」に置換
```bash
sed 's/apple/orange/g' file.txt
```
行ごと削除（3行目を削除）
```bash
sed '3d' file.txt
```

### **7. grep（パターン検索）**
「error」を含む行を表示
```bash
grep 'error' log.txt
```
大文字・小文字を無視
```bash
grep -i 'error' log.txt
```
行番号付き
```bash
grep -n 'error' log.txt
```
正規表現を使用（例：「error」または「fail」）
```bash
grep -E 'error|fail' log.txt
```

### **8. sort（ソート）**
昇順ソート
```bash
sort file.txt
```
降順ソート
```bash
sort -r file.txt
```
数値ソート
```bash
sort -n numbers.txt
```
ユニークな行を取得
```bash
sort file.txt | uniq
```

### **9. uniq（重複行の削除）**
```bash
uniq file.txt
```
出現回数を表示
```bash
uniq -c file.txt
```

### **10. tr（文字変換）**
小文字を大文字に変換
```bash
echo "hello world" | tr 'a-z' 'A-Z'
```
スペースを改行に変換
```bash
echo "a b c" | tr ' ' '\n'
```

---

# **データ処理系コマンド**

### **1. wc（行数・単語数・バイト数をカウント）**
```bash
wc -l file.txt  # 行数
wc -w file.txt  # 単語数
wc -c file.txt  # バイト数
```

### **2. paste（列の結合）**
```bash
paste file1.txt file2.txt
```
CSVファイルのようにカンマで結合
```bash
paste -d ',' file1.txt file2.txt
```

### **3. xargs（コマンドの引数として渡す）**
```bash
echo "file1.txt file2.txt" | xargs cat
```
ファイル名のリストを渡して削除
```bash
ls *.log | xargs rm
```

### **4. find（ファイル検索）**
特定のファイルを検索
```bash
find . -name "*.txt"
```
変更から1日以内のファイルを検索
```bash
find . -mtime -1
```

### **5. tee（標準出力とファイル出力）**
```bash
ls | tee output.txt
```

### **6. split（ファイルの分割）**
```bash
split -l 1000 largefile.txt smallfile_
```

---

## **応用：複数コマンドの組み合わせ**

### **ログファイルからエラーメッセージを抽出して、上位10個を表示**
```bash
grep "error" log.txt | sort | uniq -c | sort -nr | head -10
```

### **CSVの2列目を取得してソート、重複をカウント**
```bash
cut -d ',' -f 2 file.csv | sort | uniq -c | sort -nr
```

---

# 指定した行を抽出

---

## **1. `sed` を使う（行番号指定）**
### **特定の行を抽出**
```bash
sed -n '3p' file.txt  # 3行目を表示
```
### **複数の行を抽出**
```bash
sed -n '5,10p' file.txt  # 5行目から10行目を表示
```
```bash
sed -n -e '2p' -e '5p' -e '8p' file.txt  # 2, 5, 8行目を表示
```
### **特定の行を削除（例：3行目を削除）**
```bash
sed '3d' file.txt
```

---

## **2. `awk` を使う（行番号指定）**
### **特定の行を抽出**
```bash
awk 'NR==3' file.txt  # 3行目を表示
```
### **複数の行を抽出**
```bash
awk 'NR>=5 && NR<=10' file.txt  # 5行目から10行目を表示
```
```bash
awk 'NR==2 || NR==5 || NR==8' file.txt  # 2, 5, 8行目を表示
```

---

## **3. `head` と `tail` を組み合わせる**
### **先頭から特定の行を取得**
```bash
head -n 10 file.txt  # 先頭10行を取得
```
### **末尾から特定の行を取得**
```bash
tail -n 5 file.txt  # 最後の5行を取得
```
### **5行目から10行目を抽出**
```bash
head -n 10 file.txt | tail -n 6
```

---

## **4. `sed` を使って特定の行を削除**
```bash
sed '3d' file.txt  # 3行目を削除
```
```bash
sed '5,10d' file.txt  # 5行目から10行目を削除
```

---

## **5. `grep` を使う（行番号指定はできないが、特定の文字列を含む行を抽出）**
```bash
grep 'error' file.txt  # "error" を含む行を抽出
```
```bash
grep -n 'error' file.txt  # 行番号付きで表示
```

---

### **応用：ランダムな行を抽出**
```bash
shuf -n 1 file.txt  # ランダムに1行抽出
```

# 指定した列を抽出  

---

## **1. `cut` を使う（シンプルな列抽出）**
`cut` コマンドは、特定の列を簡単に取得できます。

### **CSVやTSVの特定の列を抽出**
```bash
cut -d ',' -f 2 file.csv  # CSVの2列目を取得
cut -d $'\t' -f 3 file.tsv  # TSVの3列目を取得（タブ区切り）
```

### **複数の列を抽出**
```bash
cut -d ',' -f 1,3 file.csv  # 1列目と3列目を取得
```

### **区切り文字を変更**
デフォルトでは `cut` はスペースやタブを扱えませんが、 `-d` オプションで変更可能。
```bash
cut -d ' ' -f 2 file.txt  # スペース区切りの2列目を取得
```

---

## **2. `awk` を使う（高度な列処理）**
`awk` は列の抽出だけでなく、計算やフィルタリングも可能。

### **特定の列を抽出**
```bash
awk -F ',' '{print $2}' file.csv  # CSVの2列目を取得
awk -F '\t' '{print $3}' file.tsv  # TSVの3列目を取得
```

### **複数の列を抽出**
```bash
awk -F ',' '{print $1, $3}' file.csv  # 1列目と3列目を取得
```

### **条件付きで列を抽出**
例：3列目の値が50以上の行の2列目を表示
```bash
awk -F ',' '$3 >= 50 {print $2}' file.csv
```

---

## **3. `grep` と `cut` を組み合わせる**
特定のキーワードを含む行から列を抽出
```bash
grep "error" log.txt | cut -d ' ' -f 2
```

---

## **4. `perl` を使う**
`perl` で高速に列を抽出可能。
```bash
perl -F, -lane 'print $F[1]' file.csv  # 2列目を取得（0-based index）
```

---

## **5. `awk` でヘッダー付きの列を抽出**
```bash
awk -F ',' 'NR==1 {print $2}; NR>1 {print $2}' file.csv
```

---

### **応用例：CSVから2列目と3列目をソートして表示**
```bash
cut -d ',' -f 2,3 file.csv | sort
```

---

# 列の値でソート

---

## **1. `sort` コマンドで指定列をソート**  

`sort` コマンドはデフォルトでは行全体をソートしますが、 `-k` オプションで列を指定できます。  

### **数値としてソート（昇順）**
```bash
sort -t ',' -k2,2n file.csv  # 2列目を数値として昇順ソート（CSV）
sort -t $'\t' -k3,3n file.tsv  # 3列目を数値として昇順ソート（TSV）
```

### **数値としてソート（降順）**
```bash
sort -t ',' -k2,2nr file.csv  # 2列目を数値として降順ソート
```

### **文字列としてソート（辞書順）**
```bash
sort -t ',' -k2,2 file.csv  # 2列目を文字列として昇順ソート
```

### **列の一部だけを使ってソート**
例えば、 `2023-01-15` のような日付が2列目にある場合、`YYYY-MM` だけでソートするには:
```bash
sort -t ',' -k2.1,2.7 file.csv  # 2列目の1文字目から7文字目でソート
```

---

## **2. `awk` + `sort` を組み合わせてソート**
`awk` で指定列を抽出し、 `sort` で並べ替える。

### **CSVの2列目でソート**
```bash
awk -F ',' '{print $2, $0}' file.csv | sort -k1,1n | cut -d ' ' -f2-
```
**説明**
- `awk -F ',' '{print $2, $0}'` → 2列目を先頭にして全行出力
- `sort -k1,1n` → 先頭（2列目の値）で数値ソート
- `cut -d ' ' -f2-` → ソートのために追加した2列目の値を削除

---

## **3. `sort` でヘッダーを保持したままソート**
ファイルの最初の行（ヘッダー）を保持してソートするには:
```bash
{ head -n 1 file.csv; tail -n +2 file.csv | sort -t ',' -k2,2n; } > sorted.csv
```
**説明**
- `head -n 1 file.csv` → 1行目（ヘッダー）をそのまま保持
- `tail -n +2 file.csv | sort -t ',' -k2,2n` → 2行目以降を2列目でソート
- `{ ... } > sorted.csv` → 結果を `sorted.csv` に保存

---

## **4. `sort` で日付をソート**
もし2列目が `YYYY-MM-DD` の日付なら:
```bash
sort -t ',' -k2,2d file.csv
```
または、日付を適切に扱うには `-M` オプション（ `Jan, Feb, ...` のような月名の場合）:
```bash
sort -t ',' -k2,2M file.csv  # "Jan", "Feb" 形式の月でソート
```

---

## **5. `sort` で複数列の優先順位を指定してソート**
例えば、2列目で昇順ソートし、同じ値なら3列目で降順ソート:
```bash
sort -t ',' -k2,2n -k3,3nr file.csv
```

---

### **応用例**
#### **CSVの3列目の値が50以上の行を抽出してソート**
```bash
awk -F ',' '$3 >= 50' file.csv | sort -t ',' -k3,3nr
```

---

# 正規表現（regex）を使ったテキスト処理  

---

## **1. `grep` を使う（パターン検索）**  
`grep` は指定したパターンにマッチする行を抽出します。  

### **基本的な検索**
```bash
grep 'error' file.txt  # "error" を含む行を表示
grep -i 'error' file.txt  # 大文字・小文字を無視して検索
```

### **正規表現を使った検索**
#### **特定の単語を検索**
```bash
grep -E '\berror\b' file.txt  # "error" のみを検索（単語全体に一致）
```

#### **複数のキーワードを検索**
```bash
grep -E 'error|fail|warning' file.txt
```

#### **行頭・行末を指定**
```bash
grep '^ERROR' file.txt  # "ERROR" で始まる行
grep 'failed$' file.txt  # "failed" で終わる行
```

#### **数字や特定の形式を検索**
```bash
grep -E '[0-9]{4}-[0-9]{2}-[0-9]{2}' file.txt  # "YYYY-MM-DD" の日付フォーマット
```

---

## **2. `sed` を使う（文字列の置換）**
`sed` を使うと、正規表現でマッチした部分を置換できます。  

### **特定の単語を置換**
```bash
sed 's/error/failure/g' file.txt  # "error" を "failure" に置換
```

### **行の削除**
```bash
sed '/^#/d' file.txt  # "#" で始まるコメント行を削除
```

### **数字をすべて削除**
```bash
sed 's/[0-9]//g' file.txt
```

### **空行を削除**
```bash
sed '/^$/d' file.txt
```

---

`sed` のデフォルトでは **基本正規表現（BRE）** を使用し、`-E` オプションで **拡張正規表現（ERE）** が使えます。  


### **1. 基本的なパターン**
| パターン | 意味 | 例 |
|----------|------|----|
| `.` | 任意の1文字 | `s/.*/Hello/g` |
| `^` | 行の先頭 | `s/^Hello/Hi/g` |
| `$` | 行の末尾 | `s/world!$/Linux!/g` |
| `*` | 直前の文字を0回以上繰り返す | `s/lo*/L/g` (`loooo` → `L`) |
| `\+` | 直前の文字を1回以上繰り返す | `s/e\+/E/g` (`tree` → `trE`) |
| `\?` | 直前の文字を0回または1回 | `s/colou\?r/color/g` (`colour` → `color`) |
| `\{n,m\}` | n回以上m回以下の繰り返し（ERE） | `s/a\{2,4\}/A/g` (`aaa` → `A`) |
| `[]` | 文字クラス（任意の1文字） | `s/[aeiou]/X/g` (`hello` → `hXllX`) |
| `[^]` | 否定（指定した以外の1文字） | `s/[^0-9]/X/g` (`abc123` → `XXX123`) |
| `\( \)` | グループ化（キャプチャ） | `s/\(Hello\) world/\1 Linux/g` |
| `\1, \2 ...` | キャプチャした部分を再利用 | `s/\([0-9]\)\([a-z]\)/\2\1/g` (`1a` → `a1`) |

---

### **2. よく使う応用パターン**
| パターン | 例 | 結果 |
|----------|----|------|
| 数字のみを削除 | `sed 's/[0-9]//g'` | `abc123` → `abc` |
| アルファベット以外を削除 | `sed 's/[^a-zA-Z]//g'` | `abc-123` → `abc` |
| 連続する空白を1つに | `sed 's/  \+/ /g'` | `Hello   world` → `Hello world` |
| `"` 以外を削除 | `sed 's/[^"]//g'` | `He said "Hello"` → `""` |
| 2つ以上の `'` を削除 | `sed "s/''\+//g"` | `Hello ''world'''` → `Hello world` |

---

### **3. `sed -E` を使った拡張正規表現（ERE）**
デフォルトの `sed` は **BRE（基本正規表現）** を使うので、  
`+`, `?`, `{}` などを使うには `-E` オプションが必要です。

```bash
echo "123abc456" | sed -E 's/[0-9]+/X/g'
```
**結果:**
```
XabcX
```
**ポイント:**
- `-E` を使うことで `\+` を `+` と省略できる
- `{n,m}`, `+`, `?` などが使いやすくなる

シングルクォート '（アポストロフィ）のコード
    16進数（Hex）: \x27
    10進数（Decimal）: 39
    Unicode（UTF-8 / UTF-16）: U+0027

このコード \x27 は、sed や awk などのツールでシングルクォートを扱う際に使えます。


---

## **3. `awk` を使う（列処理 + 正規表現）**
### **特定のパターンを含む行を抽出**
```bash
awk '/error/' file.txt  # "error" を含む行を抽出
```

### **2列目が数字の行を抽出**
```bash
awk '$2 ~ /^[0-9]+$/' file.txt
```

### **特定のパターンを置換**
```bash
awk '{gsub(/error/, "failure"); print}' file.txt
```

---

## **4. `perl` を使う（高度な正規表現）**
### **正規表現で行を抽出**
```bash
perl -ne 'print if /error|warning/' file.txt
```

### **数字だけを抽出**
```bash
perl -ne 'print "$1\n" if /([0-9]+)/' file.txt
```

---

### **応用例**
#### **ログから "error" を含む行のうち、特定の日付 (2024-01-30) のみを抽出**
```bash
grep '2024-01-30' log.txt | grep 'error'
```

#### **CSVファイルの2列目に "success" が含まれる行のみを抽出**
```bash
awk -F ',' '$2 ~ /success/' file.csv
```

---

### **具体的な正規表現の例**
| **パターン** | **意味** | **例（マッチする文字列）** |
|-------------|--------|----------------|
| `^abc` | "abc" で始まる行 | `abc123` |
| `xyz$` | "xyz" で終わる行 | `hello xyz` |
| `[0-9]` | 1桁の数字 | `123abc`（1,2,3 にマッチ） |
| `[a-zA-Z]` | 英字 | `Hello`（H,e,l,l,o にマッチ） |
| `[^0-9]` | 数字以外 | `abc!` |
| `.` | 任意の1文字 | `a#b`（"#" にマッチ） |
| `a*` | "a" が0回以上 | `aaaa`, `b`（bにもマッチ） |
| `a+` | "a" が1回以上 | `aaaa`（空文字にはマッチしない） |
| `a?` | "a" が0回または1回 | `a`, `""`（空文字） |
| `A{3,5}` | "A" が3～5回 | `AAA`, `AAAAA` |
| `(foo|bar)` | "foo" または "bar" | `foo`, `bar` |

---

### **`grep` の使い分け**
```bash
grep 'pattern' file.txt  # 基礎正規表現（BRE）
grep -E 'pattern' file.txt  # 拡張正規表現（ERE）
grep -P 'pattern' file.txt  # Perl互換正規表現（PCRE）
```

---

### **`sed` の使い分け**
```bash
sed 's/old/new/g' file.txt  # 基礎正規表現（BRE）
sed -E 's/old(new)?/replace/g' file.txt  # 拡張正規表現（ERE）
```

---

### **`awk` の使い方（デフォルトで ERE を使用）**
```bash
awk '/error/' file.txt  # "error" を含む行を表示
awk '$2 ~ /^[0-9]+$/' file.txt  # 2列目が数字の行を抽出
```

---

# **各コマンドの実行速度比較**

---

## **1. 各コマンドの基本的な処理速度の比較**
| **コマンド** | **用途** | **速度（目安）** | **特徴** |
|-------------|---------|-------------|---------|
| `grep` | 行単位の検索 | **最速** | C言語で実装され、高速 |
| `grep -E` | 拡張正規表現（ERE）検索 | **速い** | `grep` より若干遅い |
| `grep -P` | Perl互換正規表現（PCRE） | **遅い** | 高機能だが処理が重い |
| `sed` | 置換・フィルタ | **速い** | `grep` より少し遅い |
| `awk` | 列単位の処理 | **遅い** | 高度な処理が可能だが負荷が高い |
| `perl` | 高度なテキスト処理 | **遅い** | `awk` より若干高速 |
| `sort` | ソート処理 | **遅い** | O(n log n) の計算量 |

---

## **2. ベンチマーク比較（大まかな目安）**
```bash
time grep 'pattern' large_file.txt
time grep -E 'pattern' large_file.txt
time grep -P 'pattern' large_file.txt
time sed 's/pattern/replacement/g' large_file.txt
time awk '{gsub(/pattern/, "replacement"); print}' large_file.txt
```

**実行速度の目安**（ファイルサイズが1GB程度のとき）  
| コマンド | 実行時間 |
|---------|----------|
| `grep 'pattern'` | **0.5秒** |
| `grep -E 'pattern'` | **0.6秒** |
| `grep -P 'pattern'` | **3.2秒** |
| `sed 's/pattern/replacement/g'` | **1.0秒** |
| `awk '{gsub(/pattern/, "replacement"); print}'` | **1.5秒** |

---

## **3. 実行速度の最適化方法**
### **① `grep` の速度を向上**
```bash
LC_ALL=C grep 'pattern' file.txt  # ロケールを変更して高速化
grep -F 'pattern' file.txt  # 固定文字列検索（正規表現なし）で最速
```

### **② `sed` の最適化**
```bash
sed -E 's/old/new/g' file.txt > output.txt  # 余計な処理を減らす
```

### **③ `awk` の最適化**
```bash
awk '$3 ~ /pattern/' file.txt  # 必要な列だけフィルタリング
```

---

## **4. どのコマンドを使うべきか？**
| 処理内容 | 最速のコマンド |
|---------|--------------|
| 文字列検索 | `grep` (`grep -F` が最速) |
| 置換 | `sed` |
| 列処理 | `awk` |
| 正規表現（複雑） | `grep -E` または `awk` |
| Perl互換の正規表現 | `grep -P`（遅いが高機能） |

---


# 列の値でグループ化して集計

---

## **1. `sort + uniq` でグループ化してカウント**  
データの頻度（出現回数）を集計したい場合は、`sort` と `uniq` を組み合わせます。

### **例: カウント**
```bash
cat data.txt | sort | uniq -c
```
**入力 (`data.txt`)**
```
apple
banana
apple
orange
banana
banana
```

**出力**
```
   2 apple
   3 banana
   1 orange
```

**ポイント**
- `sort` → 同じ値を並べる
- `uniq -c` → 連続する値の数をカウント

### **列を指定して集計**
2列目の値ごとにグループ化する場合：
```bash
cut -d',' -f2 data.csv | sort | uniq -c
```
**`cut -d',' -f2`** → CSVの2列目を抽出  

---

## **2. `awk` でグループ化 & 集計**
より柔軟な集計をしたい場合は `awk` を使います。

### **例1: 2列目の値ごとにカウント**
```bash
awk '{count[$2]++} END {for (key in count) print key, count[key]}' data.txt
```
**入力 (`data.txt`)**
```
101 apple
102 banana
103 apple
104 orange
105 banana
106 banana
```

**出力**
```
apple 2
banana 3
orange 1
```

**説明**
- `count[$2]++` → 2列目の値ごとにカウント
- `END {for (key in count) print key, count[key]}` → 最後に結果を表示

---

### **例2: 2列目ごとの合計を計算**
2列目ごとに3列目の合計値を集計：
```bash
awk '{sum[$2] += $3} END {for (key in sum) print key, sum[key]}' data.txt
```

**入力 (`data.txt`)**
```
101 apple 10
102 banana 20
103 apple 15
104 orange 5
105 banana 30
106 banana 25
```

**出力**
```
apple 25
banana 75
orange 5
```

---

## **3. `sort + awk` で並び替えて集計**
集計後に降順に並び替え：
```bash
awk '{count[$2]++} END {for (key in count) print key, count[key]}' data.txt | sort -k2nr
```

**出力**
```
banana 3
apple 2
orange 1
```
`sort -k2nr` → 2列目を **数値降順（n）、逆順（r）** でソート

---

## **まとめ**
| 方法 | メリット | 例 |
|------|----------|------|
| `sort + uniq -c` | シンプル・速い | `sort file.txt | uniq -c` |
| `awk` | フレキシブル・計算可能 | `awk '{sum[$2]+=$3} END {for (i in sum) print i, sum[i]}' file.txt` |


# **AWK の機能一覧**

---

## **1. 基本構文**
```bash
awk '条件 { アクション }' ファイル
```
| **構成**  | **意味** |
|----------|--------|
| `条件` | どの行を処理するか指定 |
| `アクション` | その行に対して何をするか定義 |

---

## **2. フィールドとレコード**
| **要素** | **説明** |
|---------|--------|
| `$0` | 1行全体 |
| `$1, $2, ...` | 各列（フィールド） |
| `NF` | フィールド（列）の数 |
| `NR` | 行番号（現在のレコード番号） |
| `FNR` | 現在のファイルの行番号 |
| `FS` | フィールド区切り（デフォルトはスペース） |
| `OFS` | 出力時のフィールド区切り |
| `RS` | レコード区切り（デフォルトは改行） |
| `ORS` | 出力時のレコード区切り |

### **例: 各行の2列目を表示**
```bash
awk '{ print $2 }' data.txt
```
---

## **3. 条件指定（パターンマッチング）**
| **条件** | **説明** |
|---------|--------|
| `$1 == "apple"` | 1列目が `"apple"` の行を処理 |
| `$2 > 100` | 2列目が 100 より大きい行を処理 |
| `$1 ~ /regex/` | 1列目が正規表現にマッチする行を処理 |
| `$1 !~ /regex/` | 1列目が正規表現にマッチしない行を処理 |

### **例: 3列目が50以上の行を表示**
```bash
awk '$3 >= 50' data.txt
```

---

## **4. 数値計算**
| **演算子** | **説明** |
|----------|---------|
| `+` `-` `*` `/` `%` | 四則演算 |
| `^` | べき乗 |
| `+=` `-=` `*=` `/=` | 複合代入 |
| `++` `--` | インクリメント・デクリメント |

### **例: 2列目と3列目の合計を計算**
```bash
awk '{ print $1, $2 + $3 }' data.txt
```

---

## **5. 変数と配列**
### **変数**
```awk
BEGIN { total = 0 }
{ total += $2 }
END { print "Total:", total }
```

### **連想配列（ハッシュテーブル）**
```bash
awk '{ count[$1]++ } END { for (i in count) print i, count[i] }' data.txt
```

---

## **6. ループ**
| **種類** | **構文** |
|---------|--------|
| `for` | `for (i=1; i<=NF; i++) print $i` |
| `while` | `while (i<=NF) { print $i; i++ }` |
| `do-while` | `do { print i; i++ } while (i<NF)` |

---

## **7. 条件分岐**
| **種類** | **構文** |
|---------|--------|
| `if` | `if ($2 > 100) print $1` |
| `if-else` | `if ($2 > 100) print "High"; else print "Low"` |
| `ternary` | `print ($2 > 100 ? "High" : "Low")` |

---

## **8. 文字列操作**
| **関数** | **説明** |
|---------|--------|
| `length($1)` | 文字列の長さ |
| `substr($1, 2, 3)` | 2文字目から3文字取得 |
| `tolower($1)` | 小文字変換 |
| `toupper($1)` | 大文字変換 |
| `index($1, "apple")` | 文字列 `"apple"` の位置 |

### **例: 1列目を小文字に変換**
```bash
awk '{ print tolower($1) }' data.txt
```

---

## **9. ファイル処理**
### **複数ファイルを処理**
```bash
awk '{ print FILENAME, $0 }' file1.txt file2.txt
```

### **外部ファイルを読み込む**
```bash
awk 'NR==FNR { data[$1]=$2; next } { print $1, data[$1] }' file1 file2
```

---

## **10. シェルスクリプトとの連携**
```bash
echo "hello world" | awk '{ print toupper($0) }'
```

---

## **まとめ**
| **機能** | **方法** | **例** |
|--------|--------|--------|
| フィールド取得 | `$1, $2` | `{ print $2 }` |
| 行番号 | `NR` | `{ print NR, $0 }` |
| 条件指定 | `$1 == "apple"` | `$3 > 100` |
| 計算 | `$2 + $3` | `{ print $1, $2+$3 }` |
| 文字列操作 | `toupper($1)` | `substr($1, 2, 3)` |
| 配列 | `count[$1]++` | `{ count[$1]++; END { for (i in count) print i, count[i] } }` |
| ループ | `for (i=1; i<=NF; i++)` | `while (i<=NF) { print $i; i++ }` |


# JSONを処理する

---

## **1. `jq`**
最も便利なJSON処理ツールは `jq` です。以下のコマンドでインストールできます：
```bash
sudo apt update && sudo apt install -y jq
```

### **1.1 JSONの整形（pretty-print）**

```bash
jq . data.json
```
**例（`data.json`）**
```json
{"name": "Alice", "age": 25, "city": "Tokyo"}
```
**出力**
```json
{
  "name": "Alice",
  "age": 25,
  "city": "Tokyo"
}
```

---

### **1.2 JSONの特定のキーを抽出**
```bash
jq '.name' data.json
```
**出力**
```
"Alice"
```
引用符を除去する場合：
```bash
jq -r '.name' data.json
```
**出力**
```
Alice
```

---

### **1.3 ネストした値を取得**
```json
{
  "user": {
    "name": "Alice",
    "details": {
      "age": 25,
      "city": "Tokyo"
    }
  }
}
```
**取得コマンド**
```bash
jq '.user.details.city' data.json
```
**出力**
```
"Tokyo"
```

---

### **1.4 配列の処理**
```json
{
  "users": [
    {"name": "Alice", "age": 25},
    {"name": "Bob", "age": 30},
    {"name": "Charlie", "age": 22}
  ]
}
```
#### **全ユーザーの名前を取得**
```bash
jq '.users[].name' data.json
```
**出力**
```
"Alice"
"Bob"
"Charlie"
```

#### **年齢が25歳以上のユーザーを抽出**
```bash
jq '.users[] | select(.age >= 25)' data.json
```

#### **名前と年齢をリスト化**
```bash
jq '.users[] | {name: .name, age: .age}' data.json
```

---

### **1.5 JSONのフィルタリング・ソート**
#### **年齢でソート**
```bash
jq '.users | sort_by(.age)' data.json
```
#### **出力をタブ区切りにする**
```bash
jq -r '.users[] | "\(.name)\t\(.age)"' data.json
```

---

### **1.6 JSONの変更・編集**
#### **新しいキーを追加**
```bash
jq '.users[].status = "active"' data.json
```
#### **特定のキーを削除**
```bash
jq 'del(.users[].age)' data.json
```

---

## **2. `python -m json.tool`（整形のみ）**
簡単な整形ならPython標準の `json.tool` を使う：
```bash
cat data.json | python3 -m json.tool
```
または
```bash
python3 -m json.tool < data.json
```

`json.tool` は、Python に組み込まれているモジュールで、**JSONデータの整形（pretty-print）**や**検証**を行うことができます。

## **主な機能**

### **2.1. JSONデータの整形（pretty-print）**
`json.tool` を使うと、入力された JSON データを読みやすく整形（インデント付き）して表示できます。

### **使い方**
```bash
cat data.json | python3 -m json.tool
```
または
```bash
python3 -m json.tool < data.json
```

**入力（`data.json`）**
```json
{"name":"Alice","age":25,"city":"Tokyo"}
```

**出力（整形後）**
```json
{
    "name": "Alice",
    "age": 25,
    "city": "Tokyo"
}
```

---

### **2.2. JSONデータの検証**
`json.tool` は、指定した JSON が正しい形式であるかどうかも検証します。無効な JSON を渡すと、エラーを返します。

**無効な JSON（`invalid.json`）**
```json
{"name": "Alice", "age": 25, "city": "Tokyo"
```

**エラー出力**
```bash
python3 -m json.tool < invalid.json
```
```
Expecting ',' delimiter: line 1 column 52 (char 51)
```
このエラーは、JSON の形式が間違っている（カンマが足りない、閉じ括弧がない、など）場合に表示されます。

---

## **2.3. 入力・出力のリダイレクト**
標準入力と標準出力を使って、ファイルから読み込んだり、整形後に別のファイルに書き出したりすることもできます。

**整形後の結果を別のファイルに保存**
```bash
python3 -m json.tool < data.json > formatted_data.json
```

---

## **3. `jo`（JSONを生成）**
```bash
jo name=Alice age=25 city=Tokyo
```
**出力**
```json
{"name":"Alice","age":25,"city":"Tokyo"}
```

---

## **4. `yq`（YAML ⇔ JSON変換）**
`yq` は `jq` に似ているが、YAML も扱える：
```bash
sudo apt install -y yq
yq -o=json data.yaml > data.json
```

---

## 各行に個別の JSON が含まれているファイルを処理
`jq` は行ごとに処理を行うことができます。この場合、**JSON の配列のように複数のオブジェクトを処理**する方法と、**各行を個別に処理**する方法があります。

### **シナリオ1: 各行が独立した JSON オブジェクト**
たとえば、ファイル `data.json` に次のようなデータがあるとします：

**入力 (`data.json`)**
```json
{"name": "Alice", "age": 25}
{"name": "Bob", "age": 30}
{"name": "Charlie", "age": 22}
```

各行が独立した JSON オブジェクトです。このようなデータを処理するには、`jq` を使って **`-c` オプション**（コンパクトモード）を指定し、行ごとに個別に処理します。

---

## **1. 各行を処理して結果を表示**
`jq` はデフォルトで行ごとに処理されます。つまり、1行ずつ JSON オブジェクトを解析します。

### **例: 各行の `name` を抽出する**
```bash
jq -r '.name' data.json
```

**出力**
```
Alice
Bob
Charlie
```

---

## **2. 複数の JSON を1行で処理する**
もし **複数の JSON オブジェクトを1つの配列**にまとめて処理したい場合、`jq` は `.` で全体を指定します。

### **例: 各行を配列にまとめて、すべての `name` を抽出**
```bash
jq -r '.name' data.json
```

このコマンドは、**各行**を個別に処理して `name` フィールドを抽出します。

---

## **3. 各行に対して集計・処理**
JSON オブジェクトが行ごとに並んでいる場合、集計や計算を行いたいときにも `jq` が非常に便利です。

### **例: 年齢の合計を計算する**
```bash
jq -s '[.[][] | .age] | add' data.json
```
このコマンドは、ファイルの各行を1つの配列に変換し、その後 `age` フィールドを抽出して合計します。

**出力**
```bash
77
```

### コマンドの解説

- `jq -s`:
  - `-s`（または `--slurp`）オプションは、**複数のJSONオブジェクトを配列にまとめる**ことを意味します。ファイル内の複数行の JSON を、配列の形式で処理します。

- `[.[][] | .age]`:
  - `.`: 現在の JSON オブジェクト全体を参照します。
  - `[][]`: これは **2回のアンパック**（データを展開）を意味します。`-s` オプションで JSON が配列にスラープされているので、最初に配列全体を参照し、その中の各オブジェクトに対してさらに展開します。
  - `| .age`: 各オブジェクトの `age` フィールドを抽出します。

- `| add`:
  - 最後に `add` を使って、抽出した `age` の値を **合計**します。

---

### 例: 実際にどう動作するか

#### **入力 (`data.json`)**
```json
{"name": "Alice", "age": 25}
{"name": "Bob", "age": 30}
{"name": "Charlie", "age": 22}
```

#### **コマンド**
```bash
jq -s '[.[][] | .age] | add' data.json
```

#### **処理の流れ**
1. `jq -s` によって、ファイル内の各行の JSON オブジェクトが配列にまとめられます。
   - 入力ファイルは次のように変換されます：
     ```json
     [
       {"name": "Alice", "age": 25},
       {"name": "Bob", "age": 30},
       {"name": "Charlie", "age": 22}
     ]
     ```

2. `.[][]` によって、配列内の各オブジェクトが展開され、`age` フィールドを抽出します。
   - 抽出された値は `[25, 30, 22]` です。

3. `add` によって、`age` フィールドの値が合計されます。
   - 合計は `25 + 30 + 22 = 77` です。

#### **出力**
```
77
```

もし、合計ではなく個別の値を表示したい場合は、`add` を外して単に `jq -s '[.[][] | .age]' data.json` のように使うこともできます。

---

## **シナリオ2: ファイルの中の行ごとに処理**
もし、**行ごとに特定の処理**を行いたい場合、次のようにすることもできます。

### **例: 各行の `age` が 25 以上の人だけを表示**
```bash
jq -r 'select(.age >= 25) | .name' data.json
```

**出力**
```
Alice
Bob
```

---

`jq` では、`add` 以外にもさまざまな集計や処理を行うための関数や演算子があります。以下は、`add` 以外に利用できるいくつかの関数や演算子の例です。

---

### **1. `length`**
`length` は、配列や文字列の長さを取得します。

#### **例: 配列の長さを取得**
```bash
jq '[.[][] | .age] | length' data.json
```
**入力 (`data.json`)**
```json
{"name": "Alice", "age": 25}
{"name": "Bob", "age": 30}
{"name": "Charlie", "age": 22}
```

**出力**
```
3
```
これは、`age` フィールドを抽出して、その長さ（数）を表示しています。

---

### **2. `max` / `min`**
`max` は配列内の最大値を、`min` は最小値を取得します。

#### **例: 年齢の最大値を取得**
```bash
jq '[.[][] | .age] | max' data.json
```

**出力**
```
30
```

#### **例: 年齢の最小値を取得**
```bash
jq '[.[][] | .age] | min' data.json
```

**出力**
```
22
```

---

### **3. `sum`**
`sum` は、配列内の数値の合計を計算します。`add` も同じことをしますが、`sum` はより意味的に適切です。

#### **例: 年齢の合計を計算**
```bash
jq '[.[][] | .age] | sum' data.json
```

**出力**
```
77
```

---

### **4. `group_by`**
`group_by` は、配列の要素を特定のフィールドでグループ化します。

#### **例: 名前の最初の文字でグループ化**
`group_by` は、配列内のオブジェクトを指定したキーでグループ化するために使います。ただし、`group_by` を使用する前に、配列の要素が **並び替えられている必要がある** ため、通常は **`sort_by`** と組み合わせて使用します。

- `sort_by(.name)`: 名前のアルファベット順に並べ替えます。
- `group_by(.name[0])`: 名前の最初の文字でグループ化します。

#### **入力 (`data.json`)**
```json
{"name": "Alice", "age": 25}
{"name": "Bob", "age": 30}
{"name": "Charlie", "age": 22}
{"name": "David", "age": 35}
{"name": "Eve", "age": 28}
{"name": "Evan", "age": 32}
```

```bash
jq 'sort_by(.name) | group_by(.name[0])' data.json
```

#### **出力**
```json
[
  [
    {"name": "Alice", "age": 25}
  ],
  [
    {"name": "Bob", "age": 30}
  ],
  [
    {"name": "Charlie", "age": 22}
  ],
  [
    {"name": "David", "age": 35}
  ],
  [
    {"name": "Evan", "age": 32},
    {"name": "Eve", "age": 28}
  ]
]
```

---

### **5. `select`**
`select` は、条件に一致する要素だけを抽出するフィルタリング関数です。

#### **例: 年齢が 25 歳以上の人を抽出**
```bash
jq '[.[][] | select(.age >= 25)]' data.json
```

**出力**
```json
[
  {"name": "Alice", "age": 25},
  {"name": "Bob", "age": 30}
]
```

---

### **6. `unique`**
`unique` は、配列内の重複を取り除いて、ユニークな要素を抽出します。

#### **例: 年齢のユニークな値を取得**
```bash
jq '[.[][] | .age] | unique' data.json
```

**出力**
```json
[22, 25, 30]
```

---

### **7. `map`**
`map` は、配列の各要素に関数を適用して、新しい配列を作成します。

#### **例: 年齢を 2 倍にして新しい配列を作成**
```bash
jq '[.[][] | .age] | map(. * 2)' data.json
```

**出力**
```json
[50, 60, 44]
```

---

### **8. `flatten`**
`flatten` は、配列を再帰的にフラット化します。多次元の配列を1次元に変換します。

#### **例: 配列のフラット化**
```bash
jq '[.[][] | [.age]] | flatten' data.json
```

**出力**
```json
[25, 30, 22]
```

---


```
jq 機能一覧

1. 基本的な JSON 処理

JSON 全体を整形表示       jq . input.json           インデント付きで見やすく表示
JSON の特定キーを取得	    jq '.key' input.json	    key の値を取得
ネストしたキーを取得	    jq '.parent.child' input.json   parent.child の値を取得
配列の要素を取得	        jq '.[0]' input.json	    配列の 0 番目の要素を取得
________________________________________

2. フィルタ・変換

値をテキストとして出力    jq -r '.key' input.json   ダブルクォートを除いた値を出力
JSON をフラットに表示     jq -c '.' input.json      コンパクトな 1 行 JSON 出力
データを並び替え          jq 'sort' input.json	    配列の要素をソート
特定のキーのみ表示        jq '{key1, key2}' input.json    指定したキーのみ出力
キーをリネーム            jq '{newKey: .oldKey}' input.json   oldKey を newKey に変更
________________________________________

3. 条件分岐・フィルタ

条件に一致するデータを抽出    jq 'select(.age > 20)' input.json   age が 20 より大きいものを取得
AND 条件    jq 'select(.age > 20 and .city == "Tokyo")' input.json	age > 20 かつ city が Tokyo
OR 条件     jq 'select(.age > 20 or .city == "Tokyo")' input.json	  age > 20 または city が Tokyo
________________________________________

4. 配列・オブジェクト操作

配列の長さを取得          jq 'length' input.json	        配列の要素数を取得
配列の各要素に適用	      jq 'map(.age * 2)' input.json   age の値を 2 倍にする
配列の要素をフィルタ	    jq 'map(select(.age > 20))' input.json    age > 20 の要素のみ取得
配列をフラット化	        jq 'flatten' input.json	        ネストした配列を 1 次元化
配列の要素を , で結合	    jq -r 'join(", ")' input.json   配列を , で区切った文字列に変換
________________________________________

5. 文字列操作

文字列を大文字に変換	    jq 'ascii_upcase' input.json    すべての文字を大文字に
文字列を小文字に変換	    jq 'ascii_downcase' input.json	すべての文字を小文字に
文字列を置換	            jq 'gsub("old"; "new")' input.json	"old" を "new" に置換
文字列の特定の部分を削除	jq 'gsub("\\n"; "")' input.json	    改行 (\n) を削除
________________________________________

6. 計算・集計

数値の合計	  jq 'add' input.json   配列の数値の合計を求める
最大値を取得	jq 'max' input.json   配列の最大値を取得
最小値を取得	jq 'min' input.json   配列の最小値を取得
平均値を取得	jq 'add / length' input.json    配列の平均値を計算
________________________________________

7. フォーマット変換

JSON を CSV に変換    `jq -r '[.id, .name]	@csv' input.json`
JSON を TSV に変換    `jq -r '[.id, .name]	@tsv' input.json`
JSON を YAML に変換   `jq -r '.' input.json	yq -P`
フィールドを = 形式に `jq -r 'to_entries	map("(.key)=(.value)")
________________________________________

8. ループ・関数

各要素を処理        jq 'map(. * 2)' input.json    配列の各要素を 2 倍にする
条件分岐 (if 文)    jq 'if .age > 18 then "Adult" else "Child" end' input.json
                    age > 18 なら "Adult" それ以外は "Child"
________________________________________

9. Unicode 処理

Unicode エスケープを解除    jq -r '.text' input.json    \uXXXX を通常の文字に
Unicode を削除	            jq 'gsub("\\\\u[0-9a-fA-F]{4}"; "")' input.json   \uXXXX を削除
________________________________________

10. JSONL (JSON Lines) の処理

JSON Lines を処理       jq -c '.' input.jsonl   各行をコンパクトに処理
JSON Lines を配列に変換	jq -s '.' input.jsonl   各行を配列に変換
________________________________________

11. その他便利な機能

キー一覧を取得        jq 'keys' input.json    JSON のキー一覧を取得
JSON の型を取得       jq 'type' input.json    string, array, object などを表示
JSON の深さを取得     jq 'depth' input.json   JSON のネストの深さを取得
エラーハンドリング    jq 'try .key catch "error"' input.json    .key がないと "error" を返す


@csv 機能では 自動的にカンマを適切に処理 してくれます。
________________________________________

1. 基本的な CSV 変換
JSON:
[
  {"id": 1, "name": "Alice, Bob", "age": 25},
  {"id": 2, "name": "Charlie", "age": 30}
]

jq -r '[.[] | [.id, .name, .age]] | @csv' input.json

csv
1,"Alice, Bob",25
2,Charlie,30

csv は カンマを含む文字列を自動でダブルクォート (" ") で囲む ため、CSV フォーマットが崩れない。
________________________________________

2. JSON に改行 (\n) やダブルクォート (") がある場合
JSON:
[
  {"id": 1, "name": "Alice\nBob", "comment": "She said, \"Hello!\""},
  {"id": 2, "name": "Charlie", "comment": "No issues"}
]

jq -r '[.[] | [.id, .name, .comment]] | @csv' input.json

csv
1,"Alice
Bob","She said, ""Hello!"""
2,Charlie,"No issues"

処理内容:
1.	\n (改行) はそのまま適用されるため、CSV の改行がズレることがある。
2.	" (ダブルクォート) を含む場合、自動的に "" にエスケープされる。
________________________________________

3. JSON 内の改行 (\n) を削除
JSON の \n (改行) を削除し、CSV を安全にするには gsub("\\n"; " ") を使う：
jq -r '[.[] | [.id, (.name | gsub("\\n"; " ")), (.comment | gsub("\\n"; " "))]] | @csv' input.json

csv
1,"Alice Bob","She said, ""Hello!"""
2,Charlie,"No issues"
改行 (\n) をスペース ( ) に変換し、フォーマットを崩さない！
________________________________________

4. JSON に null がある場合
JSON:
[
  {"id": 1, "name": "Alice", "email": null},
  {"id": 2, "name": "Bob", "email": "bob@example.com"}
]

jq -r '[.[] | [.id, .name, .email]] | @csv' input.json

csv
1,Alice,
2,Bob,bob@example.com
null は 空白 として扱われる。
明示的に NULL という文字を入れる場合:
jq -r '[.[] | [.id, .name, (.email // "NULL")]] | @csv' input.json

csv
1,Alice, NULL
2,Bob,bob@example.com
.email // "NULL" で null を "NULL" に置き換え。
________________________________________

5. CSV 出力の列順を指定
JSON のキー順に関わらず、特定の順番で CSV に出力 したい場合：
jq -r '[.[] | [.name, .id, .email]] | @csv' input.json
name, id, email の順で CSV を作成。
________________________________________

6. tsv (タブ区切り) に変換
タブ (\t) 区切りの TSV にする場合：
jq -r '[.[] | [.id, .name, .email]] | @tsv' input.json
@tsv を使うことで @csv と同様にエスケープ処理を適用しつつ、タブ区切りで出力。

```



# XMLを処理する

`xmlstarlet` は XML を操作するためのコマンドラインツールで、XML の解析、変換、クエリの実行、編集などが可能です。以下に基本的な使い方を紹介します。

```sh
sudo apt install xmlstarlet
```

---

## 1. XML ファイルを整形 (`format` or `fo`)
XML を読みやすい形に整形できます。

```sh
xmlstarlet fo input.xml
```

インデント 2スペース
```sh
xmlstarlet fo -s 2 input.xml
```

---

## 2. XML をクエリ (`select` or `sel`)
XPath を使用して XML の特定の部分を取得できます。

```sh
xmlstarlet sel -t -v "//book/title" input.xml
```
上記は `<book>` 要素内の `<title>` の内容を出力します。

出力に改行を入れる場合：
```sh
xmlstarlet sel -t -m "//book/title" -v "." -n input.xml
```

---

## 3. XML を編集 (`edit` or `ed`)
XML の要素を変更、追加、削除できます。

### 要素の値を変更：
```sh
xmlstarlet ed -u "//book/title" -v "New Title" input.xml
```

### 要素を追加：
```sh
xmlstarlet ed -s "//book" -t elem -n "author" -v "John Doe" input.xml
```

### 要素を削除：
```sh
xmlstarlet ed -d "//book/price" input.xml
```

---

## 4. XML をカウント
特定の要素の数を数える場合：

```sh
xmlstarlet sel -t -v "count(//book)" input.xml
```

---

## 5. XML を CSV に変換
```sh
xmlstarlet sel -T -t -m "//book" -v "title" -o "," -v "author" -n input.xml
```
これは `<book>` の `title` と `author` をカンマ区切りで出力します。

---

### まとめ
| コマンド  | 説明 |
|-----------|------|
| `fo` | 整形 (フォーマット) |
| `sel` | XPath クエリで選択 |
| `tr` | XSLT 変換 |
| `ed` | 編集 |
| `val` | 検証 |
| `count` | 要素数をカウント |



## wikipediaダンプファイルを処理する

Wikipedia の XML ダンプファイルは、メディアウィキのデータを保存するための XML 形式のファイルです。ダンプにはさまざまな種類がありますが、基本的な構造は共通しています。

---

### **Wikipedia の XML ダンプの基本構造**
Wikipedia の XML ダンプ（例：`enwiki-latest-pages-articles.xml.bz2`）の基本的な構造は以下のようになっています。

```xml
<mediawiki xmlns="http://www.mediawiki.org/xml/export-0.10/"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
           xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd"
           version="0.10"
           xml:lang="en">
    <siteinfo>
        <sitename>Wikipedia</sitename>
        <dbname>enwiki</dbname>
        <base>https://en.wikipedia.org/wiki/Main_Page</base>
        <generator>MediaWiki 1.41.0-wmf.10</generator>
        <case>first-letter</case>
        <namespaces>
            <namespace key="-2">Media</namespace>
            <namespace key="-1">Special</namespace>
            <namespace key="0" />
            <namespace key="1">Talk</namespace>
            <!-- 他の名前空間が続く -->
        </namespaces>
    </siteinfo>

    <page>
        <title>Example Page</title>
        <ns>0</ns>
        <id>12345</id>
        <revision>
            <id>67890</id>
            <parentid>67889</parentid>
            <timestamp>2025-02-07T12:34:56Z</timestamp>
            <contributor>
                <username>ExampleUser</username>
                <id>54321</id>
            </contributor>
            <comment>Example edit summary</comment>
            <model>wikitext</model>
            <format>text/x-wiki</format>
            <text xml:space="preserve">{{Infobox example}}</text>
            <sha1>abcdef1234567890</sha1>
        </revision>
    </page>

    <page>
        <!-- 別のページのデータ -->
    </page>

</mediawiki>
```


以下に、Wikipedia の XML ダンプを模倣した **XML サンプル** を作成しました。このサンプルには **複数のページ** が含まれており、各ページに対して `id`、`title`、`revision`（`text`）の要素が含まれています。

#### **XML サンプル**

```xml
<?xml version="1.0" encoding="UTF-8"?>
<mediawiki xmlns="http://www.mediawiki.org/xml/export-0.10/"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
           xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd">
    <page>
        <title>Example Page</title>
        <id>12345</id>
        <revision>
            <id>67890</id>
            <timestamp>2025-02-07T00:00:00Z</timestamp>
            <text>'''Example''' is a sample page for demonstrating XML structure.</text>
        </revision>
    </page>
    <page>
        <title>Python (programming language)</title>
        <id>67890</id>
        <revision>
            <id>11223</id>
            <timestamp>2025-02-07T00:10:00Z</timestamp>
            <text>'''Python''' is a high-level programming language that is widely used.</text>
        </revision>
    </page>
    <page>
        <title>History of the world</title>
        <id>23456</id>
        <revision>
            <id>44567</id>
            <timestamp>2025-02-07T00:20:00Z</timestamp>
            <text>The history of the world is the history of humanity's development.</text>
        </revision>
    </page>
</mediawiki>
```

- `mediawiki` というルート要素に、各ページが `<page>` 要素として記載されています。
- 各 `<page>` 要素内には、`<title>`（ページタイトル）、`<id>`（ページID）、`<revision>`（記事のリビジョン）が含まれています。
- `<revision>` の中には、`<id>`（リビジョンID）、`<timestamp>`（タイムスタンプ）、`<text>`（記事本文）が含まれています。



---

### **2. 各要素の詳細**
| 要素 | 説明 |
|------|------|
| `<mediawiki>` | XML データのルート要素。MediaWiki のバージョン情報などを含む |
| `<siteinfo>` | Wikipedia サイトの基本情報を含む |
| `<page>` | 各 Wikipedia ページの記事データ |
| `<title>` | 記事のタイトル |
| `<ns>` | 名前空間（0 はメイン記事） |
| `<id>` | ページの一意の ID |
| `<revision>` | 記事のリビジョン情報 |
| `<id>` (revision 内) | リビジョンの一意の ID |
| `<parentid>` | 親リビジョンの ID（初回の編集には存在しない） |
| `<timestamp>` | 編集のタイムスタンプ（ISO 8601 形式） |
| `<contributor>` | 編集者情報（登録ユーザーの場合 `<username>` と `<id>`、匿名ユーザーの場合 `<ip>`） |
| `<comment>` | 編集の要約（空の場合もある） |
| `<model>` | コンテンツのモデル（通常は `wikitext`） |
| `<format>` | 形式（通常は `text/x-wiki`） |
| `<text>` | 実際の記事のウィキテキスト（ページの本文） |
| `<sha1>` | 編集内容の SHA-1 ハッシュ値 |

---

### **3. XML の例：Wikipedia の記事**
以下は、Wikipedia の「Python (programming language)」ページの例です。

```xml
<page>
    <title>Python (programming language)</title>
    <ns>0</ns>
    <id>23862</id>
    <revision>
        <id>12004567</id>
        <parentid>12004321</parentid>
        <timestamp>2025-02-07T14:00:00Z</timestamp>
        <contributor>
            <username>ExampleEditor</username>
            <id>67890</id>
        </contributor>
        <comment>Added information about new Python version</comment>
        <model>wikitext</model>
        <format>text/x-wiki</format>
        <text xml:space="preserve">'''Python''' is a programming language...</text>
        <sha1>9abcd123ef567890</sha1>
    </revision>
</page>
```

---

### **4. Wikipedia ダンプの種類**
Wikipedia はさまざまな XML ダンプを提供しています。主なダンプの種類は次のとおりです。

| ファイル名 | 内容 |
|-----------|------|
| `pages-articles.xml.bz2` | 記事の本文と履歴の最終リビジョン |
| `pages-meta-history.xml.bz2` | 記事の全履歴 |
| `pages-logging.xml.bz2` | ログ情報（削除、保護、ブロック履歴など） |
| `siteinfo-namespaces.xml` | 名前空間情報 |

Wikipedia のダンプデータは [Wikimedia Dumps](https://dumps.wikimedia.org/) で入手できます。

---


### **名前空間の問題**
Wikipedia の XML ダンプファイルでは、通常 XML に名前空間（namespace）が含まれています。`xmlstarlet` はデフォルトでは名前空間を無視しますが、名前空間を適切に処理する必要があります。

例えば、Wikipedia の XML ダンプには `mediawiki` 名前空間が含まれており、`<page>` や `<title>` 要素にも名前空間が付いていることがあります。これを無視すると、タグが一致せずに結果が表示されません。


`xmlstarlet` で名前空間を扱うには、`-N` オプションを使って名前空間を指定する必要があります。


```sh
# see in xml <mediawiki xmlns="http://www.mediawiki.org/xml/export-0.11/", mw is "http://www.mediawiki.org/xml/export-0.11/"

# add header
echo 'id,title,text' > enwiki01.csv

xmlstarlet sel -T -N mw="http://www.mediawiki.org/xml/export-0.11/" -t -m "//mw:page"\
 -v "mw:id"\
 -o "," -v "mw:title"\
 -o ",\"" -v "translate(normalize-space(mw:revision/mw:text), '\n', ' ')" -o "\""\
 -n enwiki01.xml |\
 LC_ALL=c sed -E '
  s/\{\{[^}]+\}\}//g;
  s/#REDIRECT .*/"/;
  s/[^0-9A-Za-z,.()"\x27]/ /g;
  s/\x27\x27\+/ /g;
  s/  +/ /g
 ' >> enwiki01.csv

export LC_ALL=ja_JP.UTF-8
```

1. `-T` オプション: タグなしのシンプルな出力を指定しています。
2. `-N mw="http://www.mediawiki.org/xml/export-0.10/"`: 名前空間 `mw` を指定しており、XML 内の要素に名前空間を考慮しています。
3. `-t`: タグなしで出力するオプションです。
4. `-m "//mw:page"`: `page` 要素を指定してその中からデータを抽出するマッチパターンです。
5. `-v 'mw:id' -o ',' -v 'mw:title' -o ',' -v 'mw:revision/text'`: 各要素 (`id`, `title`, `text`) を抽出して、カンマ区切りで出力するよう指定しています。
6. `-n`: 各行の後に改行を追加します。
7. 元のXMLにカンマ（,）が含まれている場合、CSVに出力すると崩れます。
この場合、sel の -o オプションを使い、各値を " で囲みます。
8. 元のXMLに 改行コード（\n や \r\n）が含まれている場合、そのままCSVに出力すると崩れます。
この問題、normalize-space() を使うと、改行や余分なスペースを 1つのスペース に変換できます。



このコマンドを実行すると、`---.xml` ファイルの中から、各ページの `id`、`title`、`text` をカンマ区切りで抽出した CSV が出力されます。出力例としては次のような形になります：

```
12345,Example Page,'''Example''' is a sample page for demonstrating XML structure.
67890,Python (programming language),'''Python''' is a high-level programming language that is widely used.
23456,History of the world,The history of the world is the history of humanity's development.
```



### wikipedia分割ファイルをCSVにする
```sh
#page html
wget -q https://dumps.wikimedia.org/enwiki/latest/

#bz2 file link list
wget -q -O - https://dumps.wikimedia.org/enwiki/latest/ | grep -oE 'href="[^"]+\.bz2"' | sed -E 's/href="([^"]+)"/https:\/\/dumps.wikimedia.org\/enwiki\/latest\/\1/'


#get article bz2 file link list
mkdir part
cd part
wget -q -O - https://dumps.wikimedia.org/enwiki/latest/ | grep -oE 'href="enwiki-latest-pages-articles[0-9]{1,2}[^"]*\.bz2"' | sed -E 's/href="([^"]+)"/https:\/\/dumps.wikimedia.org\/enwiki\/latest\/\1/' > bz2list.txt

#download all bz2 file from the list
wget -i bz2list.txt

#unzip all bz2 file to xml file
bzip2 -d *.bz2

#extract id, title, text from xml and make csv files
for xml in *; do
  csv="${xml/.xml/}.csv" # csv file name from xml file
  echo 'id,title,text' > "$csv"

  xmlstarlet sel -T -N mw="http://www.mediawiki.org/xml/export-0.11/" -t -m "//mw:page" \
    -v "mw:id" \
    -o "," -v "mw:title" \
    -o "," -v "concat('\"', translate(normalize-space(mw:revision/mw:text), '\"', ''), '\"')" \
    -n "$xml" | \
  sed -E '
    s/<style[^>]*>.*<\/style>//g;
    s/<[^>]+>//g;
    s/\{\{[^}]+\}\}//g;
    /#REDIRECT/d;
    /," *"/d;
    s/[^0-9A-Za-z,.()&%?"\x27-]/ /g;
    s/\x27\x27+/ /g;
    s/  +/ /g
  ' | \
    tr 'A-Z' 'a-z' >> "$csv"
done

export LC_ALL="$OLD_LC_ALL"

#marge all csv files
echo "id,title,text" > ../enwiki.csv
tail -n +2 -q *.csv >> ../enwiki.csv

```


### wikipedia全体ファイルをメモリ節約しながらSQLiteにする
```sh
wget https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles-multistream1.xml.bz2 -O enwiki.xml.bz2

  > enwiki.xml.bz2

bzip2 -d enwiki.xml.bz2
  > enwiki.xml

sudo snap install yq

#split large xml by wikipedia page tag

mkdir part
cd part

awk 'BEGIN {count=0;} 
     /<page>/ {count++; filename=sprintf("part_%08d.xml", count); print "<mediawiki>" > filename} 
     {if (count > 0) print $0 >> filename} 
     /<\/page>/ {print "</mediawiki>" >> filename; close(filename)}' ../enwiki.xml

ls | wc -l
ls | tail -n 10

find . -name "part*.xml" |\
xargs -n 1000 yq -p=xml -o=json |\
jq -c '{id: .mediawiki.page.id, title: .mediawiki.page.title, text: .mediawiki.page.revision.text."+content"}' >> ../enwiki.json

 > enwiki.json

sudo apt install sqlite3

sqlite3 enwiki.db <<EOF
CREATE TABLE IF NOT EXISTS enwiki (id,title,text);
EOF
```

json2sqlite.py
```python

	import sqlite3
	import json
	
	# SQLiteデータベースに接続
	conn = sqlite3.connect("enwiki01.db")
	cursor = conn.cursor()
	
	# JSONLファイルを開いて1行ずつ読み込む
	with open("enwiki01.json", "r", encoding="utf-8") as file:
    	for line in file:
        	data = json.loads(line.strip())  # JSONを辞書に変換
        	cursor.execute("INSERT INTO enwiki10 (id, title, text) VALUE>
                       	(int(data["id"]), data["title"], data["text>
	
	# コミットして接続を閉じる
	conn.commit()
	conn.close()


python3 json2sqlite.py

sqlite3 enwiki.db

SELECT id, title, REPLACE(text, char(10), '') FROM enwiki limit 5;
```

json2sqlite.js (node.js)
```javascript

  const fs = require("fs");
  const Database = require("better-sqlite3");
  
  // データベース接続
  const db = new Database("enwiki01.db");
  db.pragma("journal_mode = WAL"); // 書き込み高速化
  
  // テーブル作成（存在しない場合）
  db.exec(`
    CREATE TABLE IF NOT EXISTS enwiki (
      id INTEGER PRIMARY KEY,
      title TEXT NOT NULL,
      text TEXT NOT NULL
    );
  `);
  
  // 挿入用のプリペアドステートメント
  const insert = db.prepare("INSERT INTO enwiki (id, title, text) VALUES (?, ?, ?)");
  
  // トランザクションを使った一括挿入
  const insertMany = db.transaction((records) => {
    for (const record of records) {
      insert.run(record.id, record.title, record.text);
    }
  });
  
  // JSONL（1行ずつのJSON）をストリームで処理
  const stream = fs.createReadStream("enwiki01.json", { encoding: "utf8" });
  
  let buffer = "";
  let batch = [];
  const BATCH_SIZE = 1000; // 一度に処理するレコード数
  
  stream.on("data", (chunk) => {
    buffer += chunk;
    let lines = buffer.split("\n");
  
    // 最後の行は不完全な可能性があるので buffer に保持
    buffer = lines.pop();
  
    for (const line of lines) {
      if (line.trim()) {
        try {
          const data = JSON.parse(line);
          batch.push(data);
          if (batch.length >= BATCH_SIZE) {
            insertMany(batch);
            batch = []; // バッチをリセット
          }
        } catch (err) {
          console.error("JSON Parse Error:", err);
        }
      }
    }
  });
  
  // ファイル読み込み完了時
  stream.on("end", () => {
    if (batch.length > 0) {
      insertMany(batch); // 残ったデータを処理
    }
    console.log("✅ データベースへの挿入が完了しました！");
    db.close();
  });
  
  // エラーハンドリング
  stream.on("error", (err) => {
    console.error("File Read Error:", err);
  });
```
